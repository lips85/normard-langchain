{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# 시험 결과 embedding, llm 성능 모두 openai가 더 좋았음\n",
    "# 또한 ollama는 openai보다 더 느림\n",
    "# ollama는 embedding이 매우 잘못되어 있음... 이유를 모르겠음... (제대로 확인해 볼 문제... 내가 설정하는 과정이 문제일 수 있음)\n",
    "\n",
    "LLM_model, models = [\"openai\", \"GPT-3.5-turbo\"]\n",
    "# LLM_model, models = [\"ollama\", \"openhermes:latest\"]\n",
    "\n",
    "file_name = \"document.txt\"\n",
    "\n",
    "llm = (\n",
    "    ChatOllama(temperature=0.1, model=models)\n",
    "    if LLM_model == \"ollama\"\n",
    "    else ChatOpenAI(temperature=0.1)\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(f\"./files/{file_name}\")\n",
    "cache_dir = LocalFileStore(f\"./.cache/embeddings/{LLM_model}/{models}/{file_name}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separators=[\"\\n\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = (\n",
    "    OllamaEmbeddings(model=models) if LLM_model == \"ollama\" else OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are an AI that reads documents for you. Please answer based on the document given below. \n",
    "            If the information is not in the document, answer the question with “The required information is not in the document.” Never make up answers. \\n\\n{context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document does not provide information on whether Aaronson is guilty or not.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The message he wrote on the table was: \n",
      "\n",
      "FREEDOM IS SLAVERY\n",
      "TWO AND TWO MAKE FIVE\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia is a character mentioned in the document. She is someone who is significant to the protagonist and has a complex relationship with him.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first question you asked was \"Is Aaronson guilty?\"\n",
      "The second question you asked was \"What message did he write in the table?\"\n",
      "The third question you asked was \"Who is Julia?\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What was the first question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the second question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the third question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Is Aaronson guilty?'), AIMessage(content='The document does not provide information on whether Aaronson is guilty or not.'), HumanMessage(content='What message did he write in the table?'), AIMessage(content='The message he wrote on the table was: \\n\\nFREEDOM IS SLAVERY\\nTWO AND TWO MAKE FIVE'), HumanMessage(content='Who is Julia?'), AIMessage(content='Julia is a character mentioned in the document. She is someone who is significant to the protagonist and has a complex relationship with him.'), HumanMessage(content='What was the first question I asked?'), AIMessage(content='The required information is not in the document.'), HumanMessage(content='What was the second question I asked?'), AIMessage(content='The required information is not in the document.'), HumanMessage(content='What was the third question I asked?'), AIMessage(content='The required information is not in the document.'), HumanMessage(content='What was the first question I asked?'), AIMessage(content='The first question you asked was \"Is Aaronson guilty?\"'), HumanMessage(content='What was the second question I asked?'), AIMessage(content='The second question you asked was \"What message did he write in the table?\"'), HumanMessage(content='What was the third question I asked?'), AIMessage(content='The third question you asked was \"Who is Julia?\"')]), return_messages=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
