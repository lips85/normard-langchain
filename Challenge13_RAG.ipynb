{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given text, there is no information available to determine if Aaronson is guilty or innocent.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 로컬 LLM 부분이니 무시하셔요 (아래 두개)\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "# LLM_model, models = [\"openai\", \"GPT-3.5-turbo\"]\n",
    "LLM_model, models = [\"ollama\", \"openhermes:latest\"]\n",
    "\n",
    "file_name = \"document.txt\"\n",
    "\n",
    "llm = (\n",
    "    ChatOllama(temperature=0.1, model=models)\n",
    "    if LLM_model == \"ollama\"\n",
    "    else ChatOpenAI(temperature=0.1)\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm, max_token_limit=120, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\\n\", chunk_size=600, chunk_overlap=100\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = (\n",
    "    OllamaEmbeddings(model=models) if LLM_model == \"ollama\" else OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "    Return any relevant text verbatim.\n",
    "    -----\n",
    "    {context}\n",
    "    \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# context = extracted parts of a long document. 도큐멘트의 요약본\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"¥n¥n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "    Given the following extracted parts of a long document and a question,create a final answer.\n",
    "    If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "    ---\n",
    "    {context}\n",
    "    \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriver,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": map_chain,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text provided does not mention any message written in a table. Therefore, we cannot determine what message, if any, was written in the table by Aaronson.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia is Winston Smith's lover and fellow member of the Brotherhood, an underground resistance group against the totalitarian government in George Orwell's novel \"1984.\" However, she is not directly mentioned or introduced in the provided text.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first question you asked was: \"Is Aaronson guilty?\" However, there is no information available to determine if Aaronson is guilty or innocent based on the given text.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What was the first question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Is Aaronson guilty?'), AIMessage(content='Based on the given text, there is no information available to determine if Aaronson is guilty or innocent.'), HumanMessage(content='What message did he write in the table?'), AIMessage(content='The text provided does not mention any message written in a table. Therefore, we cannot determine what message, if any, was written in the table by Aaronson.'), HumanMessage(content='Who is Julia?'), AIMessage(content='Julia is Winston Smith\\'s lover and fellow member of the Brotherhood, an underground resistance group against the totalitarian government in George Orwell\\'s novel \"1984.\" However, she is not directly mentioned or introduced in the provided text.'), HumanMessage(content='What was the first question I asked?'), AIMessage(content='The first question you asked was: \"Is Aaronson guilty?\" However, there is no information available to determine if Aaronson is guilty or innocent based on the given text.')]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
