{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "The document does not provide information about Aaronson's guilt.\n",
      "\n",
      "chain.invoke(question)\n",
      "content='The document mentions that Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.' response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 3111, 'total_tokens': 3133}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "type(chain.invoke(question))\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# 시험 결과 embedding, llm 성능 모두 openai가 더 좋았음\n",
    "# 또한 ollama는 openai보다 더 느림\n",
    "# ollama는 embedding이 매우 잘못되어 있음... 이유를 모르겠음... (제대로 확인해 볼 문제... 내가 설정하는 과정이 문제일 수 있음)\n",
    "\n",
    "LLM_model, models = [\"openai\", \"GPT-3.5-turbo\"]\n",
    "# LLM_model, models = [\"ollama\", \"openhermes:latest\"]\n",
    "\n",
    "file_name = \"document.txt\"\n",
    "\n",
    "llm = (\n",
    "    ChatOllama(temperature=0.1, model=models)\n",
    "    if LLM_model == \"ollama\"\n",
    "    else ChatOpenAI(temperature=0.1)\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(f\"./files/{file_name}\")\n",
    "cache_dir = LocalFileStore(f\"./.cache/embeddings/{LLM_model}/{models}/{file_name}\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separators=[\"\\n\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = (\n",
    "    OllamaEmbeddings(model=models) if LLM_model == \"ollama\" else OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are an AI that reads documents for you. Please answer based on the document given below. \n",
    "            If the information is not in the document, answer the question with “The required information is not in the document.” Never make up answers. \\n\\n{context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result},\n",
    "    )\n",
    "    print(\"result\")\n",
    "    print(result)\n",
    "    print(\"\")\n",
    "    print(\"chain.invoke(question)\")\n",
    "    print(chain.invoke(question))\n",
    "    print(\"\")\n",
    "    print(\"type(chain.invoke(question))\")\n",
    "    print(type(chain.invoke(question)))\n",
    "\n",
    "\n",
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"Is Aaronson guilty?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What message did he write in the table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"Who is Julia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chain(\"What was the first question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the second question I asked?\")\n",
    "\n",
    "invoke_chain(\"What was the third question I asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
